{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98a3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.applications import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f29d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_frames_and_labels(videos_path, activities, frame_size=(224, 224), n_frames=16):\n",
    "    frames = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, activity in enumerate(activities):\n",
    "        activity_folder = os.path.join(videos_path, activity)\n",
    "        video_files = os.listdir(activity_folder)\n",
    "        \n",
    "        for video_file in video_files:\n",
    "            video_path = os.path.join(activity_folder, video_file)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            step = frame_count // n_frames\n",
    "\n",
    "            for j in range(n_frames):\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame = cv2.resize(frame, frame_size)\n",
    "                    frames.append(frame)\n",
    "                    labels.append(i)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, (j + 1) * step)\n",
    "            cap.release()\n",
    "            \n",
    "    return np.array(frames), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab4270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import os\n",
    "\n",
    "# project_dir = os.path.abspath(\"./\")\n",
    "# os.chdir(project_dir)\n",
    "\n",
    "videos_path = \"../data/Sport Videos\"\n",
    "activities = [\"BaseballPitch\", \"Basketball\", \"Fencing\", \"TennisSwing\", \"VolleyballSpiking\"]\n",
    "frame_size = (224, 224)\n",
    "n_frames = 16\n",
    "\n",
    "frames, labels = load_video_frames_and_labels(videos_path, activities, frame_size, n_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6b655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(frames, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=len(activities))\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=len(activities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222e1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data augmentation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_datagen.fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0c4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using transfer learning\n",
    "\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\n",
    "\n",
    "# Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(len(activities), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171b07df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 13:21:54.511452: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 511s 2s/step - loss: 0.5446 - accuracy: 0.8062 - val_loss: 0.1063 - val_accuracy: 0.9784\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 516s 2s/step - loss: 0.1360 - accuracy: 0.9600 - val_loss: 0.0377 - val_accuracy: 0.9931\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 467s 2s/step - loss: 0.0752 - accuracy: 0.9797 - val_loss: 0.0227 - val_accuracy: 0.9977\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 470s 2s/step - loss: 0.0555 - accuracy: 0.9857 - val_loss: 0.0150 - val_accuracy: 0.9982\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 473s 2s/step - loss: 0.0398 - accuracy: 0.9902 - val_loss: 0.0128 - val_accuracy: 0.9982\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 469s 2s/step - loss: 0.0279 - accuracy: 0.9926 - val_loss: 0.0105 - val_accuracy: 0.9982\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 456s 2s/step - loss: 0.0253 - accuracy: 0.9940 - val_loss: 0.0107 - val_accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 453s 2s/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 0.0087 - val_accuracy: 0.9982\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 449s 2s/step - loss: 0.0168 - accuracy: 0.9950 - val_loss: 0.0062 - val_accuracy: 0.9991\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 440s 2s/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.0053 - val_accuracy: 0.9986\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train) // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1ad000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 86s 1s/step - loss: 0.0053 - accuracy: 0.9986\n",
      "Validation loss: 0.0052587841637432575\n",
      "Validation accuracy: 0.9986238479614258\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "\n",
    "scores = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(\"Validation loss:\", scores[0])\n",
    "print(\"Validation accuracy:\", scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3e3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"video_classification_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c3ac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 244s 892ms/step\n",
      "69/69 [==============================] - 75s 1s/step\n"
     ]
    }
   ],
   "source": [
    "def extract_features(model, frames, batch_size=32):\n",
    "    features = model.predict(frames, batch_size=batch_size, verbose=1)\n",
    "    return features\n",
    "\n",
    "# Load the base model\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\n",
    "\n",
    "# Extract features for the training and validation sets\n",
    "X_train_features = extract_features(base_model, X_train)\n",
    "X_val_features = extract_features(base_model, X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a71f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features(features, n_frames=16):\n",
    "    aggregated_features = []\n",
    "    for i in range(0, len(features), n_frames):\n",
    "        avg_features = np.mean(features[i:i + n_frames], axis=0)\n",
    "        aggregated_features.append(avg_features)\n",
    "    return np.array(aggregated_features)\n",
    "\n",
    "X_train_aggregated = aggregate_features(X_train_features)\n",
    "X_val_aggregated = aggregate_features(X_val_features)\n",
    "\n",
    "# Adjust the labels to match the aggregated features\n",
    "y_train_aggregated = y_train[::16]\n",
    "y_val_aggregated = y_val[::16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e58f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 1.8558 - accuracy: 0.2202 - val_loss: 1.6864 - val_accuracy: 0.1825\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.7405 - accuracy: 0.2569 - val_loss: 1.5738 - val_accuracy: 0.2336\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.7161 - accuracy: 0.2495 - val_loss: 1.5851 - val_accuracy: 0.2701\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.6624 - accuracy: 0.2752 - val_loss: 1.5876 - val_accuracy: 0.2701\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.6761 - accuracy: 0.2220 - val_loss: 1.5496 - val_accuracy: 0.3066\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.5901 - accuracy: 0.3046 - val_loss: 1.6051 - val_accuracy: 0.2701\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.6373 - accuracy: 0.2477 - val_loss: 1.5663 - val_accuracy: 0.3066\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 1.5809 - accuracy: 0.2734 - val_loss: 1.5568 - val_accuracy: 0.3285\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.5698 - accuracy: 0.2826 - val_loss: 1.5536 - val_accuracy: 0.2993\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 1.5608 - accuracy: 0.3009 - val_loss: 1.5854 - val_accuracy: 0.2774\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train_aggregated.shape[1:]\n",
    "\n",
    "classifier = keras.Sequential([\n",
    "    Dense(512, activation='relu', input_shape=input_shape),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(activities), activation='softmax')\n",
    "])\n",
    "\n",
    "classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = classifier.fit(X_train_aggregated, y_train_aggregated,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          validation_data=(X_val_aggregated, y_val_aggregated),\n",
    "                          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0ae9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 939us/step - loss: 1.5854 - accuracy: 0.2774\n",
      "Validation loss: 1.5853623151779175\n",
      "Validation accuracy: 0.277372270822525\n"
     ]
    }
   ],
   "source": [
    "scores = classifier.evaluate(X_val_aggregated, y_val_aggregated, verbose=1)\n",
    "print(\"Validation loss:\", scores[0])\n",
    "print(\"Validation accuracy:\", scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04c6012",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"video_classification_model_simplified.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7702c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "272/272 [==============================] - 1205s 4s/step - loss: 0.0694 - accuracy: 0.9777 - val_loss: 0.0055 - val_accuracy: 0.9977\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 1336s 5s/step - loss: 0.0155 - accuracy: 0.9964 - val_loss: 0.0309 - val_accuracy: 0.9922\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 1360s 5s/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 0.0190 - val_accuracy: 0.9940\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 3635s 13s/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0020 - val_accuracy: 0.9995\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 1285s 5s/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 7.5348e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 1381s 5s/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.0335 - val_accuracy: 0.9872\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 1406s 5s/step - loss: 0.0095 - accuracy: 0.9978 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 1676s 6s/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.9198e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 1346s 5s/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 1323s 5s/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 1.2409e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train a CNN for image classification on the frames\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "\n",
    "image_classifier = keras.Sequential([\n",
    "    base_model,\n",
    "    Dense(len(activities), activation='softmax')\n",
    "])\n",
    "\n",
    "image_classifier.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = image_classifier.fit(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                                steps_per_epoch=len(X_train) // batch_size,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4adbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply moving average on predictions:\n",
    "\n",
    "def moving_average(predictions, window_size=5):\n",
    "    cumsum = np.cumsum(predictions, axis=0)\n",
    "    cumsum[window_size:] = cumsum[window_size:] - cumsum[:-window_size]\n",
    "    return cumsum[window_size - 1:] / window_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99160d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 77s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Predict frame-level probabilities and apply the moving average:\n",
    "\n",
    "frame_probabilities = image_classifier.predict(X_val)\n",
    "averaged_probabilities = moving_average(frame_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cf7a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the predictions to video-level by averaging frame predictions for each video:\n",
    "\n",
    "def aggregate_probabilities(probabilities, n_frames=16):\n",
    "    aggregated_probabilities = []\n",
    "    for i in range(0, len(probabilities), n_frames):\n",
    "        avg_probabilities = np.mean(probabilities[i:i + n_frames], axis=0)\n",
    "        aggregated_probabilities.append(avg_probabilities)\n",
    "    return np.array(aggregated_probabilities)\n",
    "\n",
    "y_val_aggregated_probabilities = aggregate_probabilities(averaged_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "652e3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the aggregated probabilities to class labels:\n",
    "\n",
    "y_val_predicted = np.argmax(y_val_aggregated_probabilities, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f778914b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [137, 136]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      5\u001b[0m y_val_true \u001b[38;5;241m=\u001b[39m y_val[::\u001b[38;5;241m16\u001b[39m]  \u001b[38;5;66;03m# Use the original labels of the validation set, taking one label per video\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_predicted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "File \u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [137, 136]"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_val_true = y_val[::16]  # Use the original labels of the validation set, taking one label per video\n",
    "accuracy = accuracy_score(y_val_true, y_val_predicted)\n",
    "print(\"Validation accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919ee67",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880feff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(image_path, activities, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for activity_id, activity in enumerate(activities):\n",
    "        activity_folder = os.path.join(image_path, activity)\n",
    "        for image_name in os.listdir(activity_folder):\n",
    "            image = cv2.imread(os.path.join(activity_folder, image_name))\n",
    "            image = cv2.resize(image, image_size)\n",
    "            images.append(image)\n",
    "            labels.append(activity_id)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./data/Sport Images\"\n",
    "activities = [\"baseball\", \"basketball\", \"fencing\", \"tennis\", \"volleyball\"]\n",
    "image_size = (224, 224)\n",
    "\n",
    "images, labels = load_images_and_labels(image_path, activities, image_size)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(images, labels, test_size=0.15, stratify=labels, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, stratify=y_train_val, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e2e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding:\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=len(activities))\n",
    "y_val = to_categorical(y_val, num_classes=len(activities))\n",
    "y_test = to_categorical(y_test, num_classes=len(activities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the transfer learning model:\n",
    "\n",
    "def create_transfer_learning_model(base_model, num_classes):\n",
    "    x = base_model.output\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data augmentation:\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193114f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the training and validation errors:\n",
    "\n",
    "def plot_errors(history, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model for each pre-trained model:\n",
    "\n",
    "pretrained_models = [ResNet50, EfficientNetB0, VGG16]\n",
    "pretrained_model_names = ['ResNet50', 'EfficientNetB0', 'VGG16']\n",
    "\n",
    "for model_id, pretrained_model in enumerate(pretrained_models):\n",
    "    # Load pre-trained model\n",
    "    base_model = pretrained_model(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\n",
    "\n",
    "    # Create transfer learning model\n",
    "    model = create_transfer_learning_model(base_model, len(activities))\n",
    "\n",
    "    # Freeze all layers except the last Dense layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with data augmentation\n",
    "    batch_size = 5\n",
    "    epochs = 100\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    history = model.fit(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=len(X_train) // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=1)\n",
    "\n",
    "    # Plot the training and validation errors vs. epochs\n",
    "    plot_errors(history, pretrained_model_names[model_id])\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_train_pred = np.argmax(model.predict(X_train), axis=1)\n",
    "    y_val_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "    y_test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    y_train_true = np.argmax(y_train, axis=1)\n",
    "    y_val_true = np.argmax(y_val, axis=1)\n",
    "    y_test_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(f\"Results for {pretrained_model_names[model_id]}:\")\n",
    "    print(\"Confusion Matrix (Train):\")\n",
    "    print(confusion_matrix(y_train_true, y_train_pred))\n",
    "    print(\"Confusion Matrix (Validation):\")\n",
    "    print(conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a0caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
